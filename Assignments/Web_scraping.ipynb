{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c4cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f75d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "#Question 1 Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "#https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame.\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 100:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    movies = []\n",
    "    movie_items = soup.find_all('div', class_='lister-item mode-detail')\n",
    "    for item in movie_items:\n",
    "        name = item.h3.a.text.strip()\n",
    "        year = item.h3.find('span', class_='lister-item-year text-muted unbold').text.strip('() ')\n",
    "        rating_tag = item.find('span', class_='ipl-rating-star__rating')\n",
    "        rating = float(rating_tag.text.strip()) if rating_tag else 'N/A'\n",
    "        movies.append({\n",
    "            'Name': name,\n",
    "            'Year of Release': year,\n",
    "            'Rating': rating\n",
    "        })\n",
    "    df = pd.DataFrame(movies)\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03499e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "#heading, date, content and the likes for the video from the link for the youtube video from the post.\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "session = requests.Session()\n",
    "response = session.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('article') \n",
    "    for post in posts:\n",
    "        heading = post.find('h2').text if post.find('h2') else 'No Heading'\n",
    "        date = post.find('time')['datetime'] if post.find('time') else 'No Date'\n",
    "        content = post.find('p').text if post.find('p') else 'No Content'\n",
    "        likes = post.find('span', class_='likes-count').text if post.find('span', class_='likes-count') else 'No Likes'\n",
    "        video_link = ''\n",
    "        for link in post.find_all('a', href=True):\n",
    "            if 'youtube.com' in link['href']:\n",
    "                video_link = link['href']\n",
    "                break\n",
    "        print(f\"Heading: {heading}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        print(f\"Likes: {likes}\")\n",
    "        print(f\"YouTube Video Link: {video_link}\")\n",
    "        print('-' * 50)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7e17a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "#heading, date, content and the likes for the video from the link for the youtube video from the post.\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "session = requests.Session()\n",
    "response = session.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('article') \n",
    "    \n",
    "    for post in posts:\n",
    "        heading = post.find('h2').text if post.find('h2') else 'No Heading'\n",
    "        date = post.find('time')['datetime'] if post.find('time') else 'No Date'\n",
    "        content = post.find('p').text if post.find('p') else 'No Content'\n",
    "        likes = post.find('span', class_='likes-count').text if post.find('span', class_='likes-count') else 'No Likes'\n",
    "        video_link = ''\n",
    "        for link in post.find_all('a', href=True):\n",
    "            if 'youtube.com' in link['href']:\n",
    "                video_link = link['href']\n",
    "                break\n",
    "        print(f\"Heading: {heading}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        print(f\"Likes: {likes}\")\n",
    "        print(f\"YouTube Video Link: {video_link}\")\n",
    "        print('-' * 50)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b037a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping houses for Indira Nagar...\n",
      "Scraping houses for Jayanagar...\n",
      "Scraping houses for Rajaji Nagar...\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Question 3 Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "#area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "#Rajaji Nagar.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def scrape_houses(locality):\n",
    "    base_url = f\"https://www.nobroker.in/property/rent/{locality.replace(' ', '%20')}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data for {locality}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    property_cards = soup.find_all('div', class_='card')\n",
    "    houses = []\n",
    "    for card in property_cards:\n",
    "        try:\n",
    "            title = card.find('span', class_='card-title').text.strip()\n",
    "           \n",
    "            location = card.find('div', class_='nb__nWzqU').text.strip()\n",
    "            \n",
    "            area = card.find('div', class_='nb__sajfl').text.strip()\n",
    "            \n",
    "            emi = card.find('div', class_='nb__1WIbx').text.strip() if card.find('div', class_='nb__1WIbx') else 'N/A'\n",
    "            \n",
    "            price = card.find('div', class_='nb__tTUkA').text.strip() if card.find('div', class_='nb__tTUkA') else 'N/A'\n",
    "            \n",
    "            houses.append({\n",
    "                'Title': title,\n",
    "                'Location': location,\n",
    "                'Area': area,\n",
    "                'EMI': emi,\n",
    "                'Price': price\n",
    "            })\n",
    "        \n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return houses\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "all_houses = []\n",
    "\n",
    "for locality in localities:\n",
    "    print(f\"Scraping houses for {locality}...\")\n",
    "    houses = scrape_houses(locality)\n",
    "    all_houses.extend(houses)\n",
    "df = pd.DataFrame(all_houses)\n",
    "print(df)\n",
    "df.to_csv('nobroker_houses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "331ce3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4 Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "#https://www.bewakoof.com/bestseller?sort=popular \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_names = []\n",
    "    product_prices = []\n",
    "    product_images = []\n",
    "    products = soup.find_all('div', class_='productCardBox', limit=10)  \n",
    "\n",
    "    for product in products:\n",
    "        \n",
    "        name = product.find('h3', class_='productCardDetail').get_text(strip=True)\n",
    "        price = product.find('span', class_='discountedPriceText').get_text(strip=True)\n",
    "        image_url = product.find('img', class_='productImgTag')['src']\n",
    "        product_names.append(name)\n",
    "        product_prices.append(price)\n",
    "        product_images.append(image_url)\n",
    "    df = pd.DataFrame({\n",
    "        'Product Name': product_names,\n",
    "        'Price': product_prices,\n",
    "        'Image URL': product_images\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d5eb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline Date  \\\n",
      "0   Bull market limps into 2-year birthday with so...  N/A   \n",
      "1   Small nuclear reactors could power the future ...  N/A   \n",
      "2   Bull market limps into 2-year birthday with so...  N/A   \n",
      "3   'Get Britain building again': New UK finance c...  N/A   \n",
      "4   Britain's Labour pulled off a thumping electio...  N/A   \n",
      "5   UK's Labour Party secures landslide victory in...  N/A   \n",
      "6   Labour does not have much headroom in terms of...  N/A   \n",
      "7   Outgoing UK PM Rishi Sunak to step down as Con...  N/A   \n",
      "8   World swelters through its hottest summer on r...  N/A   \n",
      "9   Water shortages are brewing future wars — with...  N/A   \n",
      "10  Sweden's Volvo Cars scraps plan to sell only e...  N/A   \n",
      "11  ESG investors appear to be getting much more c...  N/A   \n",
      "12  Finland will soon bury spent nuclear fuel in t...  N/A   \n",
      "13  Passengers often ignore in-flight safety video...  N/A   \n",
      "14  Has Boeing shaken your confidence to fly? A ne...  N/A   \n",
      "15  Interest to visit Japan soars, while China str...  N/A   \n",
      "16  Can a robot give a decent massage? We tested o...  N/A   \n",
      "17  This ship travels the world full time — here's...  N/A   \n",
      "18  The 10 colleges with the happiest students—App...  N/A   \n",
      "19  Harvard nutritionist: The No. 1 ultra-processe...  N/A   \n",
      "20  Covid-19, flu, RSV: Vaccine recommendations fo...  N/A   \n",
      "21  The payout option experts recommend if you win...  N/A   \n",
      "22  How to master your money: Practical strategies...  N/A   \n",
      "23                     What are the economics of war?  N/A   \n",
      "24                    What is the internet of bodies?  N/A   \n",
      "25       How the world got into $315 trillion of debt  N/A   \n",
      "26  eVTOLS: Are flying cars finally becoming reality?  N/A   \n",
      "27                  How China's property bubble burst  N/A   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2024/09/07/bull-market-li...  \n",
      "1   https://www.cnbc.com/2024/09/07/how-small-modu...  \n",
      "2   https://www.cnbc.com/2024/09/07/bull-market-li...  \n",
      "3   https://www.cnbc.com/2024/07/08/uk-election-20...  \n",
      "4   https://www.cnbc.com/2024/07/05/uk-election-20...  \n",
      "5   https://www.cnbc.com/video/2024/07/05/uks-labo...  \n",
      "6   https://www.cnbc.com/video/2024/07/05/labour-d...  \n",
      "7   https://www.cnbc.com/2024/07/05/uk-election-ri...  \n",
      "8   https://www.cnbc.com/2024/09/06/climate-crisis...  \n",
      "9   https://www.cnbc.com/2024/09/05/water-wars-fla...  \n",
      "10  https://www.cnbc.com/2024/09/04/swedens-volvo-...  \n",
      "11  https://www.cnbc.com/2024/09/02/esg-sustainabl...  \n",
      "12  https://www.cnbc.com/2024/08/29/onkalo-finland...  \n",
      "13  https://www.cnbc.com/2024/09/05/youtube-video-...  \n",
      "14  https://www.cnbc.com/2024/09/02/fear-of-flying...  \n",
      "15  https://www.cnbc.com/2024/08/27/why-travel-dem...  \n",
      "16  https://www.cnbc.com/2024/08/22/a-robot-that-g...  \n",
      "17  https://www.cnbc.com/2024/08/21/ship-that-trav...  \n",
      "18  https://www.cnbc.com/2024/09/07/colleges-with-...  \n",
      "19  https://www.cnbc.com/2024/09/07/harvard-nutrit...  \n",
      "20  https://www.cnbc.com/2024/09/06/covid-19-flu-r...  \n",
      "21  https://www.cnbc.com/2024/09/06/payout-option-...  \n",
      "22  https://www.cnbc.com/2024/07/10/achieve-financ...  \n",
      "23  https://www.cnbc.com/video/2024/08/07/what-are...  \n",
      "24  https://www.cnbc.com/video/2024/05/31/what-is-...  \n",
      "25  https://www.cnbc.com/video/2024/05/28/how-the-...  \n",
      "26  https://www.cnbc.com/video/2024/03/28/evtols-h...  \n",
      "27  https://www.cnbc.com/video/2024/02/29/how-chin...  \n"
     ]
    }
   ],
   "source": [
    "#Question 5 Please visit https://www.cnbc.com/world/?region=world and scrapaheadingsb) datec) News link\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the CNBC World News page\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headlines = []\n",
    "    dates = []\n",
    "    links = []\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer') \n",
    "    \n",
    "    for article in articles:\n",
    "        headline = article.find('a').get_text(strip=True)\n",
    "        link = article.find('a')['href']\n",
    "        date = article.find('time')\n",
    "        if date:\n",
    "            date = date.get_text(strip=True)\n",
    "        else:\n",
    "            date = 'N/A'\n",
    "        headlines.append(headline)\n",
    "        dates.append(date)\n",
    "        links.append(link)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Headline': headlines,\n",
    "        'Date': dates,\n",
    "        'News Link': links\n",
    "    })\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c025fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6 Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticleand scrap\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/\"\n",
    "\n",
    "# Send a GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the relevant sections\n",
    "articles = soup.find_all('div', class_='article')\n",
    "\n",
    "for article in articles:\n",
    "    # Extract the title, date, and authors\n",
    "    title = article.find('h2', class_='article-title').text.strip()\n",
    "    date = article.find('span', class_='article-date').text.strip()\n",
    "    authors = article.find('div', class_='article-authors').text.strip()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Authors: {authors}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c58b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4590c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
