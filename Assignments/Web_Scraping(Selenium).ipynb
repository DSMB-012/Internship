{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305726c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#Install a selenium library\n",
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ee0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location.\n",
    "#You have to scrape the job-title, job-location, company_name, experience_required. You have to\n",
    "#scrape first 10 jobs data.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "# connect to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "#driver.close()\n",
    "driver.get('https://www.naukri.com/')\n",
    "#finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name('suggestor-input ')\n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()\n",
    "#alternate way to open the same page\n",
    "#specifying the url of the webpage to be scraped\n",
    "#url=\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\"\n",
    "#driver.get(url)\n",
    "job_titles=[]\n",
    "company_names=[]\n",
    "locations_list=[]\n",
    "experience_list=[]\n",
    "#lets extract all the tags having the job titles\n",
    "titles_tags=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "titles_tags[0:4]\n",
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:4]\n",
    "companies_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "companies_tags[0:4]\n",
    "for i in companies_tags:\n",
    "    title=i.text\n",
    "    company_names.append(title)\n",
    "company_names[0:4]\n",
    "experience_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span[1]')\n",
    "experience_tags[0:4]\n",
    "for i in experience_tags:\n",
    "    title=i.text\n",
    "    experience_list.append(title)\n",
    "experience_list[0:4]\n",
    "location_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span[1]')\n",
    "location_tags[0:4]\n",
    "for i in location_tags:\n",
    "    title=i.text\n",
    "    locations_list.append(title)\n",
    "locations_list[0:4]\n",
    "print(len(job_titles),len(company_names),len(experience_list),len(locations_list))\n",
    "jobs=pd.DataFrame()\n",
    "jobs['Titles']=job_titles[0:10]\n",
    "jobs['company']=company_names[0:10]\n",
    "jobs['experience_required']=experience_list[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe10fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "#Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\n",
    "#job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Enter search criteria and click search button\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "payload = {\n",
    "  \"search_query\": job_title,\n",
    "  \"loc_query\": location\n",
    "}\n",
    "response = requests.post(url, data=payload)\n",
    "\n",
    "# Step 3: Scrape the data for the first 10 jobs\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "job_results = soup.find_all(\"div\", class_=\"result-display\")\n",
    "job_data = []\n",
    "for result in job_results[:10]:\n",
    "    title = result.find(\"h2\").text.strip()\n",
    "    company = result.find(\"span\", class_=\"company-name\").text.strip()\n",
    "    location = result.find(\"span\", class_=\"location\").text.strip()\n",
    "    job_data.append({\"Job Title\": title, \"Company Name\": company, \"Location\": location})\n",
    "\n",
    "# Step 4: Create a dataframe of the scraped data\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Shine.com website and search for Data Scientist jobs in Bangalore\n",
    "driver.get(\"https://www.shine.com/job-search/data-scientist-jobs-in-bangalore\")\n",
    "\n",
    "# Wait for the page to load properly\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'search_listing')))\n",
    "\n",
    "# Scrape the first 10 job results\n",
    "job_results = driver.find_elements(By.CLASS_NAME, 'search_listing')[:10]\n",
    "\n",
    "# Loop through the job results and extract details\n",
    "for job in job_results:\n",
    "    # Scrape the job title\n",
    "    title = job.find_element(By.CSS_SELECTOR, 'li h2').text\n",
    "\n",
    "    # Scrape the location\n",
    "    location = job.find_element(By.CSS_SELECTOR, '.job_loc span').text\n",
    "\n",
    "    # Scrape the company name\n",
    "    company = job.find_element(By.CSS_SELECTOR, '.job_company span').text\n",
    "\n",
    "    # Scrape the experience required\n",
    "    experience = job.find_element(By.CSS_SELECTOR, '.exp').text\n",
    "\n",
    "    # Print the job details\n",
    "    print(f\"Job Title: {title}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Company: {company}\")\n",
    "    print(f\"Experience: {experience}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3 : Scrape 100 reviews data from flipkart.com for iphone11 phone\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Flipkart reviews page for iPhone 11\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "# Wait for the reviews section to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_1YokD2')))\n",
    "\n",
    "# Initialize an empty list to store reviews\n",
    "reviews = []\n",
    "\n",
    "# Function to scrape reviews from the current page\n",
    "def scrape_reviews():\n",
    "    review_elements = driver.find_elements(By.CLASS_NAME, 'col-12-12')\n",
    "    \n",
    "    for review in review_elements:\n",
    "        try:\n",
    "            # Rating\n",
    "            rating = review.find_element(By.CLASS_NAME, '_3LWZlK').text\n",
    "            # Review summary (title of the review)\n",
    "            review_summary = review.find_element(By.CLASS_NAME, '_2-N8zT').text\n",
    "            # Full review text\n",
    "            full_review = review.find_element(By.CLASS_NAME, 't-ZTKy').text\n",
    "\n",
    "            reviews.append({\n",
    "                'Rating': rating,\n",
    "                'Review Summary': review_summary,\n",
    "                'Full Review': full_review\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # If there is any missing element, skip this review\n",
    "            pass\n",
    "\n",
    "# Scrape reviews until we collect 100\n",
    "while len(reviews) < 100:\n",
    "    # Scrape reviews from the current page\n",
    "    scrape_reviews()\n",
    "    \n",
    "    # Check if the next button is available and clickable\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "        # Click on the 'Next' button to load more reviews\n",
    "        next_button.click()\n",
    "        # Wait for the new reviews to load\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print(\"No more pages or unable to click next.\")\n",
    "        break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Display the scraped reviews\n",
    "for i, review in enumerate(reviews[:100], start=1):\n",
    "    print(f\"Review {i}:\")\n",
    "    print(f\"Rating: {review['Rating']}\")\n",
    "    print(f\"Review Summary: {review['Review Summary']}\")\n",
    "    print(f\"Full Review: {review['Full Review']}\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4 : Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search\n",
    "field.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Flipkart website\n",
    "driver.get(\"https://www.flipkart.com\")\n",
    "\n",
    "# Close the login popup if it appears\n",
    "try:\n",
    "    close_popup = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"✕\")]'))\n",
    "    )\n",
    "    close_popup.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Search for \"sneakers\" in the search bar\n",
    "search_box = driver.find_element(By.NAME, 'q')\n",
    "search_box.send_keys('sneakers')\n",
    "search_box.submit()\n",
    "\n",
    "# Wait for the page to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_1YokD2')))\n",
    "\n",
    "# Initialize an empty list to store sneaker data\n",
    "sneakers = []\n",
    "\n",
    "# Function to scrape sneakers from the current page\n",
    "def scrape_sneakers():\n",
    "    # Find all sneaker elements on the page\n",
    "    product_elements = driver.find_elements(By.CLASS_NAME, '_1AtVbE')\n",
    "\n",
    "    for product in product_elements:\n",
    "        try:\n",
    "            # Brand\n",
    "            brand = product.find_element(By.CLASS_NAME, '_2WkVRV').text\n",
    "            # Product Description\n",
    "            product_description = product.find_element(By.CLASS_NAME, 'IRpwTa').text\n",
    "            # Price\n",
    "            price = product.find_element(By.CLASS_NAME, '_30jeq3').text\n",
    "            \n",
    "            # Add sneaker details to the list\n",
    "            sneakers.append({\n",
    "                'Brand': brand,\n",
    "                'Product Description': product_description,\n",
    "                'Price': price\n",
    "            })\n",
    "        except:\n",
    "            # Skip any product missing the required details\n",
    "            pass\n",
    "\n",
    "# Scrape sneakers until we have 100\n",
    "while len(sneakers) < 100:\n",
    "    scrape_sneakers()\n",
    "\n",
    "    # If we've collected 100 or more, break\n",
    "    if len(sneakers) >= 100:\n",
    "        break\n",
    "    \n",
    "    # Check if the next button is available and clickable\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # wait for the next page to load\n",
    "    except:\n",
    "        print(\"No more pages or unable to click next.\")\n",
    "        break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Display the scraped sneakers data\n",
    "for i, sneaker in enumerate(sneakers[:100], start=1):\n",
    "    print(f\"Sneaker {i}:\")\n",
    "    print(f\"Brand: {sneaker['Brand']}\")\n",
    "    print(f\"Product Description: {sneaker['Product Description']}\")\n",
    "    print(f\"Price: {sneaker['Price']}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to scrape sneaker data\n",
    "def scrape_flipkart_sneakers():\n",
    "    base_url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "    # Send a request to the website\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    \n",
    "    # Parse the content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all sneaker listings\n",
    "    sneakers = soup.find_all(\"div\", class_=\"_1AtVbE\")[:100]  # Limit to first 100 sneakers\n",
    "    \n",
    "    sneaker_data = []\n",
    "\n",
    "    # Loop through the sneaker listings and extract the necessary data\n",
    "    for sneaker in sneakers:\n",
    "        try:\n",
    "            # Extract brand name\n",
    "            brand = sneaker.find(\"div\", class_=\"_2WkVRV\").text\n",
    "            \n",
    "            # Extract product description\n",
    "            product_desc = sneaker.find(\"a\", class_=\"IRpwTa\").text\n",
    "            \n",
    "            # Extract price\n",
    "            price = sneaker.find(\"div\", class_=\"_30jeq3\").text\n",
    "            \n",
    "            # Store the sneaker data in a dictionary\n",
    "            sneaker_data.append({\n",
    "                \"Brand\": brand,\n",
    "                \"ProductDescription\": product_desc,\n",
    "                \"Price\": price\n",
    "            })\n",
    "        except AttributeError:\n",
    "            # Handle cases where any information is missing\n",
    "            continue\n",
    "    \n",
    "    return sneaker_data\n",
    "\n",
    "# Execute the function and print the result\n",
    "sneaker_list = scrape_flipkart_sneakers()\n",
    "\n",
    "# Print the results\n",
    "for idx, sneaker in enumerate(sneaker_list, 1):\n",
    "    print(f\"{idx}. Brand: {sneaker['Brand']}, Product Description: {sneaker['ProductDescription']}, Price: {sneaker['Price']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5 Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "#Type filter to “Intel Core i7” as shown in the below image:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (e.g., for Chrome)\n",
    "driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "\n",
    "# Open Amazon India\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "# Find the search bar and input \"Laptop\"\n",
    "search_box = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search_box.send_keys('Laptop')\n",
    "search_box.send_keys(Keys.RETURN)  # Simulate pressing 'Enter'\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.s-main-slot')))\n",
    "\n",
    "# Filter by CPU Type - \"Intel Core i7\"\n",
    "cpu_filter = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, \"//span[text()='Intel Core i7']\"))\n",
    ")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Allow the page to reload after setting the filter\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape the first 10 laptops\n",
    "laptop_data = []\n",
    "\n",
    "# Scroll down to make sure enough products are loaded\n",
    "driver.execute_script(\"window.scrollTo(0, 3000);\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find all the products listed on the page\n",
    "products = soup.find_all('div', {'data-component-type': 's-search-result'}, limit=10)\n",
    "\n",
    "for product in products:\n",
    "    # Extract title\n",
    "    title_element = product.h2.a\n",
    "    title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "    # Extract price\n",
    "    price_element = product.find('span', 'a-price-whole')\n",
    "    price = price_element.text.strip() if price_element else 'N/A'\n",
    "\n",
    "    # Extract ratings\n",
    "    rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
    "    rating = rating_element.text.strip() if rating_element else 'N/A'\n",
    "\n",
    "    # Add the extracted data to the list\n",
    "    laptop_data.append({\n",
    "        'Title': title,\n",
    "        'Price': price,\n",
    "        'Rating': rating\n",
    "    })\n",
    "\n",
    "# Print the extracted data\n",
    "for idx, laptop in enumerate(laptop_data, start=1):\n",
    "    print(f\"Laptop {idx}:\")\n",
    "    print(f\"Title: {laptop['Title']}\")\n",
    "    print(f\"Price: {laptop['Price']}\")\n",
    "    print(f\"Rating: {laptop['Rating']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Close the browser after the task is completed\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6 Write a python program to scrape data for Top 1000 Quotes of All Time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Set up the WebDriver (e.g., for Chrome)\n",
    "driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "\n",
    "# Open AZQuotes homepage\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Wait for the page to load, and click the \"Top Quotes\" button\n",
    "top_quotes_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.LINK_TEXT, \"Top Quotes\"))\n",
    ")\n",
    "top_quotes_button.click()\n",
    "\n",
    "# Wait for the Top Quotes page to load\n",
    "time.sleep(5)  # Adjust based on your network speed\n",
    "\n",
    "# Prepare an empty list to store the quotes\n",
    "quotes_data = []\n",
    "\n",
    "# Scraping multiple pages, iterate through 100 pages (as there are usually 10 quotes per page)\n",
    "for page_num in range(1, 101):  # Adjust for up to 1000 quotes (100 pages)\n",
    "    # Wait for the current page's content to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all quote containers on the page\n",
    "    quotes = soup.find_all('div', class_='wrap-block')\n",
    "    \n",
    "    for quote in quotes:\n",
    "        # Extract quote text\n",
    "        quote_text = quote.find('a', class_='title').text.strip()\n",
    "        \n",
    "        # Extract author name\n",
    "        author_text = quote.find('div', class_='author').text.strip()\n",
    "        \n",
    "        # Extract type of quote (if available)\n",
    "        type_element = quote.find('div', class_='tags')\n",
    "        quote_type = type_element.text.strip() if type_element else 'N/A'\n",
    "        \n",
    "        # Store the extracted data in the list\n",
    "        quotes_data.append({\n",
    "            'Quote': quote_text,\n",
    "            'Author': author_text,\n",
    "            'Type': quote_type\n",
    "        })\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.LINK_TEXT, \"Next\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print(f\"Failed to click 'Next' on page {page_num}\")\n",
    "        break\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "with open('top_1000_quotes.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Quote', 'Author', 'Type']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerows(quotes_data)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Print the number of quotes scraped\n",
    "print(f\"Scraped {len(quotes_data)} quotes.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
